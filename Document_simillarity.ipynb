{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('en.13.25.67.2009.11.8 Doc No. 91', 'en.13.25.458.2009.11.8 Doc No. 117'), 0.9603271710302738)\n",
      "(('en.13.25.281.2009.11.7 Doc No. 13', 'en.13.25.101.2009.11.9 Doc No. 106'), 0.9061121967688884)\n",
      "(('en.13.25.184.2009.11.9 Doc No. 48', 'en.13.25.182.2009.11.9 Doc No. 86'), 0.8791574857681683)\n",
      "(('en.13.25.239.2009.11.8 Doc No. 7', 'en.13.25.78.2009.11.8 Doc No. 112'), 0.8500616536341434)\n",
      "(('en.13.25.381.2009.11.8 Doc No. 53', 'en.13.25.392.2009.11.8 Doc No. 104'), 0.8390715005322515)\n",
      "(('en.13.25.170.2009.11.8 Doc No. 88', 'en.13.25.94.2009.11.8 Doc No. 89'), 0.8113797693569362)\n",
      "(('en.13.25.285.2009.11.8 Doc No. 68', 'en.13.25.440.2009.11.8 Doc No. 85'), 0.8077169574976977)\n",
      "(('en.13.25.399.2009.11.9 Doc No. 12', 'en.13.25.89.2009.11.9 Doc No. 87'), 0.8064823534595627)\n",
      "(('en.13.25.152.2009.11.9 Doc No. 54', 'en.13.25.252.2009.11.8 Doc No. 126'), 0.7804171169518516)\n",
      "(('en.13.25.76.2009.11.9 Doc No. 27', 'en.13.25.47.2009.11.9 Doc No. 92'), 0.7581581476551967)\n",
      "(('en.13.25.378.2009.11.8 Doc No. 44', 'en.13.25.12.2009.11.8 Doc No. 97'), 0.708958551158934)\n",
      "(('en.13.25.493.2009.11.8 Doc No. 120', 'en.13.25.50.2009.11.8 Doc No. 131'), 0.6965644421402358)\n",
      "(('en.13.25.184.2009.11.9 Doc No. 48', 'en.13.25.458.2009.11.8 Doc No. 117'), 0.673666530740311)\n",
      "(('en.13.25.213.2009.11.8 Doc No. 15', 'en.13.25.418.2009.11.9 Doc No. 24'), 0.6729659861416464)\n",
      "(('en.13.25.89.2009.11.9 Doc No. 87', 'en.13.25.78.2009.11.8 Doc No. 112'), 0.6703003321719552)\n",
      "(('en.13.25.184.2009.11.9 Doc No. 48', 'en.13.25.67.2009.11.8 Doc No. 91'), 0.6640967576413351)\n",
      "(('en.13.25.281.2009.11.7 Doc No. 13', 'en.13.25.330.2009.11.8 Doc No. 64'), 0.6543090307103615)\n",
      "(('en.13.25.182.2009.11.9 Doc No. 86', 'en.13.25.67.2009.11.8 Doc No. 91'), 0.6361725193970167)\n",
      "(('en.13.25.182.2009.11.9 Doc No. 86', 'en.13.25.458.2009.11.8 Doc No. 117'), 0.6100370875757267)\n",
      "(('en.13.25.239.2009.11.8 Doc No. 7', 'en.13.25.12.2009.11.8 Doc No. 97'), 0.6085282191525483)\n",
      "(('en.13.25.29.2009.11.8 Doc No. 46', 'en.13.25.162.2009.11.8 Doc No. 59'), 0.5981640229613718)\n",
      "(('en.13.25.330.2009.11.8 Doc No. 64', 'en.13.25.101.2009.11.9 Doc No. 106'), 0.5966387274049728)\n",
      "(('en.13.25.12.2009.11.8 Doc No. 97', 'en.13.25.78.2009.11.8 Doc No. 112'), 0.5833197512214572)\n",
      "(('en.13.25.177.2009.11.9 Doc No. 2', 'en.13.25.481.2009.11.8 Doc No. 38'), 0.5811272099534405)\n",
      "(('en.13.25.378.2009.11.8 Doc No. 44', 'en.13.25.78.2009.11.8 Doc No. 112'), 0.5791998349941135)\n",
      "(('en.13.25.399.2009.11.9 Doc No. 12', 'en.13.25.78.2009.11.8 Doc No. 112'), 0.5764989308704533)\n",
      "(('en.13.25.232.2009.11.8 Doc No. 73', 'en.13.25.26.2009.11.9 Doc No. 74'), 0.573116924170761)\n",
      "(('en.13.25.378.2009.11.8 Doc No. 44', 'en.13.25.89.2009.11.9 Doc No. 87'), 0.5700123649047978)\n",
      "(('en.13.25.281.2009.11.7 Doc No. 13', 'en.13.25.76.2009.11.9 Doc No. 27'), 0.5570857870292499)\n",
      "(('en.13.25.76.2009.11.9 Doc No. 27', 'en.13.25.101.2009.11.9 Doc No. 106'), 0.550614570167441)\n",
      "(('en.13.25.239.2009.11.8 Doc No. 7', 'en.13.25.89.2009.11.9 Doc No. 87'), 0.549561154662617)\n",
      "(('en.13.25.399.2009.11.9 Doc No. 12', 'en.13.25.378.2009.11.8 Doc No. 44'), 0.5489656100269719)\n",
      "(('en.13.25.239.2009.11.8 Doc No. 7', 'en.13.25.378.2009.11.8 Doc No. 44'), 0.5351472934385149)\n",
      "(('en.13.25.281.2009.11.7 Doc No. 13', 'en.13.25.47.2009.11.9 Doc No. 92'), 0.5237451673962346)\n",
      "(('en.13.25.325.2009.11.8 Doc No. 45', 'en.13.25.410.2009.11.9 Doc No. 122'), 0.5169657303678622)\n",
      "(('en.13.25.89.2009.11.9 Doc No. 87', 'en.13.25.12.2009.11.8 Doc No. 97'), 0.5162626344774348)\n",
      "(('en.13.25.47.2009.11.9 Doc No. 92', 'en.13.25.101.2009.11.9 Doc No. 106'), 0.5125041329883778)\n",
      "(('en.13.25.133.2009.11.8 Doc No. 28', 'en.13.25.406.2009.11.9 Doc No. 30'), 0.503024975148814)\n",
      "(('en.13.25.239.2009.11.8 Doc No. 7', 'en.13.25.399.2009.11.9 Doc No. 12'), 0.49683357162460595)\n",
      "(('en.13.25.184.2009.11.9 Doc No. 48', 'en.13.25.7.2009.11.8 Doc No. 60'), 0.4941484997102883)\n",
      "(('en.13.25.470.2009.11.8 Doc No. 5', 'en.13.25.213.2009.11.8 Doc No. 15'), 0.49380984106082165)\n",
      "(('en.13.25.399.2009.11.9 Doc No. 12', 'en.13.25.12.2009.11.8 Doc No. 97'), 0.4904633252102858)\n",
      "(('en.13.25.7.2009.11.8 Doc No. 60', 'en.13.25.458.2009.11.8 Doc No. 117'), 0.4837557391033651)\n",
      "(('en.13.25.7.2009.11.8 Doc No. 60', 'en.13.25.67.2009.11.8 Doc No. 91'), 0.4820234953274848)\n",
      "(('en.13.25.470.2009.11.8 Doc No. 5', 'en.13.25.418.2009.11.9 Doc No. 24'), 0.443162972631846)\n",
      "(('en.13.25.7.2009.11.8 Doc No. 60', 'en.13.25.182.2009.11.9 Doc No. 86'), 0.43895922440138635)\n",
      "(('en.13.25.76.2009.11.9 Doc No. 27', 'en.13.25.330.2009.11.8 Doc No. 64'), 0.4302010796033918)\n",
      "(('en.13.25.215.2009.11.8 Doc No. 42', 'en.13.25.179.2009.11.8 Doc No. 125'), 0.414865895084945)\n",
      "(('en.13.25.251.2009.11.9 Doc No. 6', 'en.13.25.381.2009.11.8 Doc No. 53'), 0.4109941608863838)\n",
      "(('en.13.25.53.2009.11.8 Doc No. 25', 'en.13.25.131.2009.11.9 Doc No. 55'), 0.4059622173563911)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = input(\"Enter directory path\")\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "doc_list = []\n",
    "# Iterate through each file\n",
    "for file_name in files:\n",
    "    doc_list.append(file_name)\n",
    "doc_dic = {}\n",
    "for doc_id, file_name in enumerate(doc_list):\n",
    "    doc_dic[file_name] = doc_id + 1\n",
    "\n",
    "\n",
    "def read_document(folder_path, document_name):\n",
    "    # Join the folder path and document name to get the full path\n",
    "    full_path = os.path.join(folder_path, document_name)\n",
    "    \n",
    "    # Read the content of the document\n",
    "    with open(full_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        return content\n",
    "\n",
    "def extract_important_content(content):\n",
    "    # Find the index of TITLE and TEXT tags\n",
    "    title_start = content.find('<TITLE>') + len('<TITLE>')\n",
    "    title_end = content.find('</TITLE>')\n",
    "    text_start = content.find('<TEXT>') + len('<TEXT>')\n",
    "    text_end = content.find('</TEXT>')\n",
    "\n",
    "    # Extract title and text\n",
    "    title = content[title_start:title_end]\n",
    "    text = content[text_start:text_end]\n",
    "\n",
    "    # Combine title and text\n",
    "    combined_content = (title + ' ' + text).strip()\n",
    "\n",
    "    # Use regular expression to find alphanumeric words with underscores\n",
    "    filtered_words = re.findall(r'\\b\\w+\\b', combined_content)\n",
    "\n",
    "    # Convert words to lowercase\n",
    "    lowercase_filtered_words = [word.lower() for word in filtered_words]\n",
    "\n",
    "    return lowercase_filtered_words\n",
    "\n",
    "def compute_word_freq(combined_content):\n",
    "    # Tokenize words and count frequencies\n",
    "    word_freq = {}\n",
    "    len_content = len(combined_content)\n",
    "    for word in combined_content:\n",
    "        # Increment frequency count\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += (1/len_content)\n",
    "        else:\n",
    "            word_freq[word] = (1/len_content)\n",
    "\n",
    "    return word_freq\n",
    "\n",
    "def compute_idf(documents, folder_path):\n",
    "    # Initialize a dictionary to store document frequency (DF) for each word\n",
    "    word_df = {}\n",
    "    total_documents = len(documents)\n",
    "    for doc_name in documents.keys():\n",
    "        # Read the document content\n",
    "        content = read_document(folder_path, doc_name)\n",
    "\n",
    "        # Extracted important content (removes contents don't have alphanumeric or underscore characters)\n",
    "        important_content_token = extract_important_content(content)\n",
    "\n",
    "        # Create a set to keep track of unique words in this document\n",
    "        unique_words_in_doc = set(important_content_token)\n",
    "\n",
    "        # Update word_df for unique words in this document\n",
    "        for unique_word in unique_words_in_doc:\n",
    "            if unique_word in word_df:\n",
    "                word_df[unique_word] += 1\n",
    "            else:\n",
    "                word_df[unique_word] = 1\n",
    "\n",
    "    # Calculate IDF for each word\n",
    "    word_idf = {}\n",
    "    for word, df in word_df.items():\n",
    "        word_idf[word] = math.log(total_documents / (df))\n",
    "    return word_idf\n",
    "\n",
    "def calculate_norm(vector):\n",
    "    sum_of_squares = 0\n",
    "    for element in vector:\n",
    "        sum_of_squares += element ** 2\n",
    "\n",
    "    euclidean_norm = sum_of_squares ** 0.5\n",
    "    return euclidean_norm\n",
    "\n",
    "def calculate_tfidf_vector(tf_dict, idf_dict):\n",
    "    tfidf_vector = {}\n",
    "    \n",
    "    for word, tf in tf_dict.items():\n",
    "        tfidf_vector[word] = tf * idf_dict[word]\n",
    "\n",
    "    # Normalize the TF-IDF vector\n",
    "    norm = calculate_norm(tfidf_vector.values())\n",
    "    if norm != 0:\n",
    "        tfidf_vector = {word: value / norm for word, value in tfidf_vector.items()}\n",
    "\n",
    "    return tfidf_vector\n",
    "\n",
    "def create_tfidf_corpus(doc_dic):\n",
    "    docs_tfidf_vector = {}\n",
    "    for doc in doc_dic.keys():\n",
    "        content = read_document(directory_path, doc)\n",
    "        content = extract_important_content(content)\n",
    "        tf_doc = compute_word_freq(content)\n",
    "        tfidf_vect = calculate_tfidf_vector(tf_doc, idf_corpus_words)\n",
    "        docs_tfidf_vector[doc] = tfidf_vect\n",
    "    return docs_tfidf_vector\n",
    "\n",
    "def calculate_cosine_similarity(documents_tfidf):\n",
    "    # Initialize a dictionary to store cosine similarity values\n",
    "    cosine_similarity_dict = {}\n",
    "\n",
    "    # Create a list of document names\n",
    "    doc_names = list(documents_tfidf.keys())\n",
    "\n",
    "    # Compute cosine similarity for each pair of documents\n",
    "    for i in range(len(doc_names)):\n",
    "        for j in range(i + 1, len(doc_names)):\n",
    "            doc1_name, doc2_name = doc_names[i], doc_names[j]\n",
    "            doc1_tfidf, doc2_tfidf = documents_tfidf[doc1_name], documents_tfidf[doc2_name]\n",
    "\n",
    "            dot_product = sum(doc1_tfidf[word] * doc2_tfidf.get(word, 0.0) for word in doc1_tfidf.keys())\n",
    "\n",
    "            # Computeing norms (magnitudes)\n",
    "            norm_doc1 = calculate_norm(doc1_tfidf.values())\n",
    "            norm_doc2 = calculate_norm(doc2_tfidf.values())\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            cosine_similarity = dot_product / (norm_doc1 * norm_doc2)\n",
    "\n",
    "            # Store the result in the dictionary\n",
    "            cosine_similarity_dict[f\"{doc1_name} Doc No. {doc_dic[doc1_name]}\", f\"{doc2_name} Doc No. {doc_dic[doc2_name]}\"] = cosine_similarity\n",
    "    return cosine_similarity_dict\n",
    "\n",
    "def top_similar_documents(num_documents):\n",
    "    docs_tfidf_vect = create_tfidf_corpus(doc_dic)\n",
    "    cosine_similarity = calculate_cosine_similarity(docs_tfidf_vect)\n",
    "    sorted_list = sorted(cosine_similarity.items(), key=lambda item: item[1], reverse=True)\n",
    "    for i in sorted_list[:num_documents]:\n",
    "        print(i)\n",
    "top_similar_documents(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('en.13.25.67.2009.11.8 Doc No. 91', 'en.13.25.458.2009.11.8 Doc No. 117'), 0.9603271710302738)\n",
      "(('en.13.25.281.2009.11.7 Doc No. 13', 'en.13.25.101.2009.11.9 Doc No. 106'), 0.9061121967688884)\n",
      "(('en.13.25.184.2009.11.9 Doc No. 48', 'en.13.25.182.2009.11.9 Doc No. 86'), 0.8791574857681683)\n",
      "(('en.13.25.239.2009.11.8 Doc No. 7', 'en.13.25.78.2009.11.8 Doc No. 112'), 0.8500616536341434)\n",
      "(('en.13.25.381.2009.11.8 Doc No. 53', 'en.13.25.392.2009.11.8 Doc No. 104'), 0.8390715005322515)\n",
      "(('en.13.25.170.2009.11.8 Doc No. 88', 'en.13.25.94.2009.11.8 Doc No. 89'), 0.8113797693569362)\n",
      "(('en.13.25.285.2009.11.8 Doc No. 68', 'en.13.25.440.2009.11.8 Doc No. 85'), 0.8077169574976977)\n",
      "(('en.13.25.399.2009.11.9 Doc No. 12', 'en.13.25.89.2009.11.9 Doc No. 87'), 0.8064823534595627)\n",
      "(('en.13.25.152.2009.11.9 Doc No. 54', 'en.13.25.252.2009.11.8 Doc No. 126'), 0.7804171169518516)\n",
      "(('en.13.25.76.2009.11.9 Doc No. 27', 'en.13.25.47.2009.11.9 Doc No. 92'), 0.7581581476551967)\n",
      "(('en.13.25.378.2009.11.8 Doc No. 44', 'en.13.25.12.2009.11.8 Doc No. 97'), 0.708958551158934)\n",
      "(('en.13.25.493.2009.11.8 Doc No. 120', 'en.13.25.50.2009.11.8 Doc No. 131'), 0.6965644421402358)\n",
      "(('en.13.25.184.2009.11.9 Doc No. 48', 'en.13.25.458.2009.11.8 Doc No. 117'), 0.673666530740311)\n",
      "(('en.13.25.213.2009.11.8 Doc No. 15', 'en.13.25.418.2009.11.9 Doc No. 24'), 0.6729659861416464)\n",
      "(('en.13.25.89.2009.11.9 Doc No. 87', 'en.13.25.78.2009.11.8 Doc No. 112'), 0.6703003321719552)\n",
      "(('en.13.25.184.2009.11.9 Doc No. 48', 'en.13.25.67.2009.11.8 Doc No. 91'), 0.6640967576413351)\n",
      "(('en.13.25.281.2009.11.7 Doc No. 13', 'en.13.25.330.2009.11.8 Doc No. 64'), 0.6543090307103615)\n",
      "(('en.13.25.182.2009.11.9 Doc No. 86', 'en.13.25.67.2009.11.8 Doc No. 91'), 0.6361725193970167)\n",
      "(('en.13.25.182.2009.11.9 Doc No. 86', 'en.13.25.458.2009.11.8 Doc No. 117'), 0.6100370875757267)\n",
      "(('en.13.25.239.2009.11.8 Doc No. 7', 'en.13.25.12.2009.11.8 Doc No. 97'), 0.6085282191525483)\n",
      "(('en.13.25.29.2009.11.8 Doc No. 46', 'en.13.25.162.2009.11.8 Doc No. 59'), 0.5981640229613718)\n",
      "(('en.13.25.330.2009.11.8 Doc No. 64', 'en.13.25.101.2009.11.9 Doc No. 106'), 0.5966387274049728)\n",
      "(('en.13.25.12.2009.11.8 Doc No. 97', 'en.13.25.78.2009.11.8 Doc No. 112'), 0.5833197512214572)\n",
      "(('en.13.25.177.2009.11.9 Doc No. 2', 'en.13.25.481.2009.11.8 Doc No. 38'), 0.5811272099534405)\n",
      "(('en.13.25.378.2009.11.8 Doc No. 44', 'en.13.25.78.2009.11.8 Doc No. 112'), 0.5791998349941135)\n",
      "(('en.13.25.399.2009.11.9 Doc No. 12', 'en.13.25.78.2009.11.8 Doc No. 112'), 0.5764989308704533)\n",
      "(('en.13.25.232.2009.11.8 Doc No. 73', 'en.13.25.26.2009.11.9 Doc No. 74'), 0.573116924170761)\n",
      "(('en.13.25.378.2009.11.8 Doc No. 44', 'en.13.25.89.2009.11.9 Doc No. 87'), 0.5700123649047978)\n",
      "(('en.13.25.281.2009.11.7 Doc No. 13', 'en.13.25.76.2009.11.9 Doc No. 27'), 0.5570857870292499)\n",
      "(('en.13.25.76.2009.11.9 Doc No. 27', 'en.13.25.101.2009.11.9 Doc No. 106'), 0.550614570167441)\n",
      "(('en.13.25.239.2009.11.8 Doc No. 7', 'en.13.25.89.2009.11.9 Doc No. 87'), 0.549561154662617)\n",
      "(('en.13.25.399.2009.11.9 Doc No. 12', 'en.13.25.378.2009.11.8 Doc No. 44'), 0.5489656100269719)\n",
      "(('en.13.25.239.2009.11.8 Doc No. 7', 'en.13.25.378.2009.11.8 Doc No. 44'), 0.5351472934385149)\n",
      "(('en.13.25.281.2009.11.7 Doc No. 13', 'en.13.25.47.2009.11.9 Doc No. 92'), 0.5237451673962346)\n",
      "(('en.13.25.325.2009.11.8 Doc No. 45', 'en.13.25.410.2009.11.9 Doc No. 122'), 0.5169657303678622)\n",
      "(('en.13.25.89.2009.11.9 Doc No. 87', 'en.13.25.12.2009.11.8 Doc No. 97'), 0.5162626344774348)\n",
      "(('en.13.25.47.2009.11.9 Doc No. 92', 'en.13.25.101.2009.11.9 Doc No. 106'), 0.5125041329883778)\n",
      "(('en.13.25.133.2009.11.8 Doc No. 28', 'en.13.25.406.2009.11.9 Doc No. 30'), 0.503024975148814)\n",
      "(('en.13.25.239.2009.11.8 Doc No. 7', 'en.13.25.399.2009.11.9 Doc No. 12'), 0.49683357162460595)\n",
      "(('en.13.25.184.2009.11.9 Doc No. 48', 'en.13.25.7.2009.11.8 Doc No. 60'), 0.4941484997102883)\n",
      "(('en.13.25.470.2009.11.8 Doc No. 5', 'en.13.25.213.2009.11.8 Doc No. 15'), 0.49380984106082165)\n",
      "(('en.13.25.399.2009.11.9 Doc No. 12', 'en.13.25.12.2009.11.8 Doc No. 97'), 0.4904633252102858)\n",
      "(('en.13.25.7.2009.11.8 Doc No. 60', 'en.13.25.458.2009.11.8 Doc No. 117'), 0.4837557391033651)\n",
      "(('en.13.25.7.2009.11.8 Doc No. 60', 'en.13.25.67.2009.11.8 Doc No. 91'), 0.4820234953274848)\n",
      "(('en.13.25.470.2009.11.8 Doc No. 5', 'en.13.25.418.2009.11.9 Doc No. 24'), 0.443162972631846)\n",
      "(('en.13.25.7.2009.11.8 Doc No. 60', 'en.13.25.182.2009.11.9 Doc No. 86'), 0.43895922440138635)\n",
      "(('en.13.25.76.2009.11.9 Doc No. 27', 'en.13.25.330.2009.11.8 Doc No. 64'), 0.4302010796033918)\n",
      "(('en.13.25.215.2009.11.8 Doc No. 42', 'en.13.25.179.2009.11.8 Doc No. 125'), 0.414865895084945)\n",
      "(('en.13.25.251.2009.11.9 Doc No. 6', 'en.13.25.381.2009.11.8 Doc No. 53'), 0.4109941608863838)\n",
      "(('en.13.25.53.2009.11.8 Doc No. 25', 'en.13.25.131.2009.11.9 Doc No. 55'), 0.4059622173563911)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "\n",
    "# Define the directory path\n",
    "directory_path = input(\"Enter directory path\")\n",
    "# List all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "doc_list = []\n",
    "# Iterate through each file\n",
    "for file_name in files:\n",
    "    doc_list.append(file_name)\n",
    "doc_dic = {}\n",
    "for doc_id, file_name in enumerate(doc_list):\n",
    "    doc_dic[file_name] = doc_id + 1\n",
    "\n",
    "\n",
    "def read_document(folder_path, document_name):\n",
    "    # Join the folder path and document name to get the full path\n",
    "    full_path = os.path.join(folder_path, document_name)\n",
    "    \n",
    "    # Read the content of the document\n",
    "    with open(full_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        return content\n",
    "\n",
    "def extract_important_content(content):\n",
    "    # Find the index of TITLE and TEXT tags\n",
    "    title_start = content.find('<TITLE>') + len('<TITLE>')\n",
    "    title_end = content.find('</TITLE>')\n",
    "    text_start = content.find('<TEXT>') + len('<TEXT>')\n",
    "    text_end = content.find('</TEXT>')\n",
    "\n",
    "    # Extract title and text\n",
    "    title = content[title_start:title_end]\n",
    "    text = content[text_start:text_end]\n",
    "\n",
    "    # Combine title and text\n",
    "    combined_content = (title + ' ' + text).strip()\n",
    "\n",
    "    # Use regular expression to find alphanumeric words with underscores\n",
    "    filtered_words = re.findall(r'\\b\\w+\\b', combined_content)\n",
    "\n",
    "    # Convert words to lowercase\n",
    "    lowercase_filtered_words = [word.lower() for word in filtered_words]\n",
    "\n",
    "    return lowercase_filtered_words\n",
    "\n",
    "def compute_word_freq(combined_content):\n",
    "    # Tokenize words and count frequencies\n",
    "    word_freq = {}\n",
    "    len_content = len(combined_content)\n",
    "    for word in combined_content:\n",
    "        # Increment frequency count\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += (1/len_content)\n",
    "        else:\n",
    "            word_freq[word] = (1/len_content)\n",
    "\n",
    "    return word_freq\n",
    "\n",
    "def compute_idf(documents, folder_path):\n",
    "    # Initialize a dictionary to store document frequency (DF) for each word\n",
    "    word_df = {}\n",
    "    total_documents = len(documents)\n",
    "    for doc_name in documents.keys():\n",
    "        # Read the document content\n",
    "        content = read_document(folder_path, doc_name)\n",
    "\n",
    "        # Extracted important content(removes contents don't have alphanumeric or underscore characters)\n",
    "        important_content_token = extract_important_content(content)\n",
    "\n",
    "        # Create a set to keep track of unique words in this document\n",
    "        unique_words_in_doc = set(important_content_token)\n",
    "\n",
    "        # Update word_df for unique words in this document\n",
    "        for unique_word in unique_words_in_doc:\n",
    "            if unique_word in word_df:\n",
    "                word_df[unique_word] += 1\n",
    "            else:\n",
    "                word_df[unique_word] = 1\n",
    "\n",
    "    # Calculate IDF for each word\n",
    "    word_idf = {}\n",
    "    for word, df in word_df.items():\n",
    "        word_idf[word] = math.log(total_documents / (df))\n",
    "    return word_idf\n",
    "\n",
    "def calculate_norm(vector):\n",
    "    sum_of_squares = 0\n",
    "    for element in vector:\n",
    "        sum_of_squares += element ** 2\n",
    "\n",
    "    euclidean_norm = sum_of_squares ** 0.5\n",
    "    return euclidean_norm\n",
    "\n",
    "def calculate_tfidf_vector(tf_dict, idf_dict):\n",
    "    tfidf_vector = {}\n",
    "    \n",
    "    for word, tf in tf_dict.items():\n",
    "        tfidf_vector[word] = tf * idf_dict[word]\n",
    "\n",
    "    # Normalize the TF-IDF vector\n",
    "    norm = calculate_norm(tfidf_vector.values())\n",
    "    if norm != 0:\n",
    "        tfidf_vector = {word: value / norm for word, value in tfidf_vector.items()}\n",
    "\n",
    "    return tfidf_vector\n",
    "\n",
    "def create_tfidf_corpus(doc_dic):\n",
    "    docs_tfidf_vector = {}\n",
    "    for doc in doc_dic.keys():\n",
    "        content = read_document(directory_path, doc)\n",
    "        content = extract_important_content(content)\n",
    "        tf_doc = compute_word_freq(content)\n",
    "        tfidf_vect = calculate_tfidf_vector(tf_doc, idf_corpus_words)\n",
    "        docs_tfidf_vector[doc] = tfidf_vect\n",
    "    return docs_tfidf_vector\n",
    "\n",
    "def calculate_cosine_similarity(documents_tfidf):\n",
    "    # Initialize a dictionary to store cosine similarity values\n",
    "    cosine_similarity_dict = {}\n",
    "\n",
    "    # Create a list of document names\n",
    "    doc_names = list(documents_tfidf.keys())\n",
    "\n",
    "    # Compute cosine similarity for each pair of documents\n",
    "    for i in range(len(doc_names)):\n",
    "        for j in range(i + 1, len(doc_names)):\n",
    "            doc1_name, doc2_name = doc_names[i], doc_names[j]\n",
    "            doc1_tfidf, doc2_tfidf = documents_tfidf[doc1_name], documents_tfidf[doc2_name]\n",
    "\n",
    "            dot_product = sum(doc1_tfidf[word] * doc2_tfidf.get(word, 0.0) for word in doc1_tfidf.keys())\n",
    "\n",
    "            # Computeing norms (magnitudes)\n",
    "            norm_doc1 = calculate_norm(doc1_tfidf.values())\n",
    "            norm_doc2 = calculate_norm(doc2_tfidf.values())\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            cosine_similarity = dot_product / (norm_doc1 * norm_doc2)\n",
    "\n",
    "            # Store the result in the dictionary\n",
    "            cosine_similarity_dict[f\"{doc1_name} Doc No. {doc_dic[doc1_name]}\", f\"{doc2_name} Doc No. {doc_dic[doc2_name]}\"] = cosine_similarity\n",
    "    return cosine_similarity_dict\n",
    "\n",
    "def top_similar_documents(num_documents):\n",
    "    docs_tfidf_vect = create_tfidf_corpus(doc_dic)\n",
    "    cosine_similarity = calculate_cosine_similarity(docs_tfidf_vect)\n",
    "    sorted_list = sorted(cosine_similarity.items(), key=lambda item: item[1], reverse=True)\n",
    "    for i in sorted_list[:num_documents]:\n",
    "        print(i)\n",
    "top_similar_documents(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
